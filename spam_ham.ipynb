{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This application is designed to classify emails between spam and ham. \n",
    "\n",
    "To do this, we proceed as follows: \n",
    "\n",
    "1. install prerequisites \n",
    "2. load and prepare CSV data - load CSV file and split into training and test datasets \n",
    "3. tokenise the message - tokenise texts with a pre-trained model \n",
    "4. create Pytorch datasets - create dataset objects for PyTorch \n",
    "5. load model and configure training - select a model for text classification (e.g. BERT) and define training parameters \n",
    "6. Train the model with the training data\n",
    "6. evaluate model - evaluate model on test data \n",
    "7. save and load model \n",
    "8. make prediction - use the model to classify new emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install prerequisites (note: it's a good practice to use anaconda and to install all packages in an environment)\n",
    "\n",
    "<span style=\"color:red\"><b>CLI command:</b> pip install torch transformers datasets scikit-learn pandas</span><br>\n",
    "<span style=\"color:red\"><b>If Applicable:</b> pip install ipywidgets</span>\n",
    "\n",
    "- torch: for training the modell\n",
    "- transformers: for the use of pre-trained models from HuggingFace\n",
    "- datasets: a helpful tool from HuggingFace for data processing and loading data sets \n",
    "- scikit-learn: for splitting the data and for evaluation metrics such as accuracy and F1-Score\n",
    "- pandas: for loading and editing CSV-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and prepare CSV\n",
    "\n",
    "Load and prepare your CSV file with the messages and categories. We will convert the data into a format that works well with HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category                                            Message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
      "Training dataset size: 4457\n",
      "Test dataset size: 1115\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split # function that splits data into training and test sets\n",
    "\n",
    "# load CSV in a pandas dataframe (= like a type of table or two-dimensional array that stores data in form of rows and columns)\n",
    "df = pd.read_csv('s_spam.csv')\n",
    "\n",
    "# display first lines from dataframe for verification\n",
    "print(df.head())\n",
    "\n",
    "# maps the category column into numerical values. Replaces category names named as ham (no spam) and spam (spam) with 0 and 1 to make them usable for machine learning\n",
    "df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})  # 'ham' -> 0, 'spam' -> 1\n",
    "\n",
    "# split into training and test set (80% training, 20% test)\n",
    "# Parameter random_state=42 ensures that the division is reproducible (i.e. the same division of the data is generated each time the code is run)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# show number of lines in the training and test set to ensure that the splitting was carried out correctly\n",
    "print(f\"Training dataset size: {len(train_df)}\")\n",
    "print(f\"Test dataset size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenisation of messages\n",
    "\n",
    "For the HuggingFace model, the texts must be converted into tokens. We use a pre-trained model such as BERT or DistilBERT, which is already available in HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer specially designed to prepare texts for input into BERT models. Converts text into tokens which can be processed by BERT model\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load Tokeniser (we use ‘bert-base-uncased’, a pre-trained model from BERT)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function for tokenising messages\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Message'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply tokenisation to the training and test data set\n",
    "train_encodings = tokenizer(list(train_df['Message']), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(test_df['Message']), truncation=True, padding=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating PyTorch datasets\n",
    "\n",
    "HuggingFace-Transformers uses dataset objects that are well compatible with PyTorch. We can create a dataset from the tokenised data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to work with tensors (multidimensional arrays) and the creation of data sets and models\n",
    "import torch\n",
    "\n",
    "# Class for customised implementation of torch.utils.data.Dataset Object which is a basis for working with PyTorch data \n",
    "class EmailDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # Saves the tokenised texts (encodings) and labels (spam/ham)\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    # Returns the example (text and label) at the specified position (idx) for use in a mini-batch\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    # Returns the number of elements in the dataset, which is necessary for the mini-batch creation\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = EmailDataset(train_encodings, list(train_df['Category']))\n",
    "test_dataset = EmailDataset(test_encodings, list(test_df['Category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load model and configure training\n",
    "\n",
    "Now we can use a model like BertForSequenceClassification from HuggingFace, which is suitable for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ccdcf8f0bc41e588f899095d05814a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7299, 'grad_norm': 10.679104804992676, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.02}\n",
      "{'loss': 0.6885, 'grad_norm': 9.798615455627441, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.04}\n",
      "{'loss': 0.6448, 'grad_norm': 9.575231552124023, 'learning_rate': 3e-06, 'epoch': 0.05}\n",
      "{'loss': 0.5767, 'grad_norm': 8.170449256896973, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.07}\n",
      "{'loss': 0.5026, 'grad_norm': 2.919818878173828, 'learning_rate': 5e-06, 'epoch': 0.09}\n",
      "{'loss': 0.3693, 'grad_norm': 5.2771711349487305, 'learning_rate': 6e-06, 'epoch': 0.11}\n",
      "{'loss': 0.3051, 'grad_norm': 4.223723411560059, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.13}\n",
      "{'loss': 0.1915, 'grad_norm': 2.7134225368499756, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.14}\n",
      "{'loss': 0.1833, 'grad_norm': 6.674505710601807, 'learning_rate': 9e-06, 'epoch': 0.16}\n",
      "{'loss': 0.0952, 'grad_norm': 3.6974830627441406, 'learning_rate': 1e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0788, 'grad_norm': 1.2570080757141113, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0579, 'grad_norm': 0.3652079701423645, 'learning_rate': 1.2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0871, 'grad_norm': 0.18576869368553162, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0586, 'grad_norm': 0.16046419739723206, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0162, 'grad_norm': 0.10875385999679565, 'learning_rate': 1.5e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0041, 'grad_norm': 0.14279805123806, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.29}\n",
      "{'loss': 0.162, 'grad_norm': 30.414087295532227, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.3}\n",
      "{'loss': 0.1435, 'grad_norm': 0.28818073868751526, 'learning_rate': 1.8e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0611, 'grad_norm': 0.4973616302013397, 'learning_rate': 1.9e-05, 'epoch': 0.34}\n",
      "{'loss': 0.1391, 'grad_norm': 0.2343534231185913, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.195, 'grad_norm': 33.600460052490234, 'learning_rate': 2.1e-05, 'epoch': 0.38}\n",
      "{'loss': 0.2042, 'grad_norm': 3.969019651412964, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0047, 'grad_norm': 0.06191283464431763, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.41}\n",
      "{'loss': 0.1253, 'grad_norm': 18.61199378967285, 'learning_rate': 2.4e-05, 'epoch': 0.43}\n",
      "{'loss': 0.148, 'grad_norm': 0.12857145071029663, 'learning_rate': 2.5e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0988, 'grad_norm': 32.17251968383789, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0788, 'grad_norm': 0.04728185012936592, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0025, 'grad_norm': 0.05298171564936638, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0709, 'grad_norm': 0.12424854189157486, 'learning_rate': 2.9e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0226, 'grad_norm': 0.06903059780597687, 'learning_rate': 3e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0665, 'grad_norm': 0.04857032746076584, 'learning_rate': 3.1e-05, 'epoch': 0.56}\n",
      "{'loss': 0.132, 'grad_norm': 0.12256688624620438, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0408, 'grad_norm': 0.04580987989902496, 'learning_rate': 3.3e-05, 'epoch': 0.59}\n",
      "{'loss': 0.1311, 'grad_norm': 1.4231901168823242, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0015, 'grad_norm': 0.019125064834952354, 'learning_rate': 3.5e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0008, 'grad_norm': 0.01228397712111473, 'learning_rate': 3.6e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0005, 'grad_norm': 0.010276523418724537, 'learning_rate': 3.7e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1761, 'grad_norm': 0.06986312568187714, 'learning_rate': 3.8e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0029, 'grad_norm': 0.053089603781700134, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0804, 'grad_norm': 0.04495582357048988, 'learning_rate': 4e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0852, 'grad_norm': 0.044427111744880676, 'learning_rate': 4.1e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0014, 'grad_norm': 0.03032912313938141, 'learning_rate': 4.2e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0932, 'grad_norm': 41.018226623535156, 'learning_rate': 4.3e-05, 'epoch': 0.77}\n",
      "{'loss': 0.046, 'grad_norm': 0.019980713725090027, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.79}\n",
      "{'loss': 0.009, 'grad_norm': 0.0854417234659195, 'learning_rate': 4.5e-05, 'epoch': 0.81}\n",
      "{'loss': 0.1117, 'grad_norm': 0.0649743601679802, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.82}\n",
      "{'loss': 0.087, 'grad_norm': 0.019031010568141937, 'learning_rate': 4.7e-05, 'epoch': 0.84}\n",
      "{'loss': 0.1327, 'grad_norm': 4.0806732177734375, 'learning_rate': 4.8e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0522, 'grad_norm': 7.022719383239746, 'learning_rate': 4.9e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0508, 'grad_norm': 0.07882805168628693, 'learning_rate': 5e-05, 'epoch': 0.9}\n",
      "{'loss': 0.081, 'grad_norm': 6.075514316558838, 'learning_rate': 4.957410562180579e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0933, 'grad_norm': 0.313044935464859, 'learning_rate': 4.914821124361158e-05, 'epoch': 0.93}\n",
      "{'loss': 0.1481, 'grad_norm': 0.10589990019798279, 'learning_rate': 4.872231686541738e-05, 'epoch': 0.95}\n",
      "{'loss': 0.1619, 'grad_norm': 0.10120535641908646, 'learning_rate': 4.829642248722317e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0706, 'grad_norm': 7.9687113761901855, 'learning_rate': 4.787052810902896e-05, 'epoch': 0.99}\n",
      "{'loss': 0.0497, 'grad_norm': 0.3617247939109802, 'learning_rate': 4.744463373083475e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0029, 'grad_norm': 0.8702554702758789, 'learning_rate': 4.701873935264054e-05, 'epoch': 1.02}\n",
      "{'loss': 0.118, 'grad_norm': 0.03953118249773979, 'learning_rate': 4.659284497444634e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0024, 'grad_norm': 0.016842443495988846, 'learning_rate': 4.6166950596252137e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0006, 'grad_norm': 0.01120784506201744, 'learning_rate': 4.574105621805793e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0005, 'grad_norm': 0.014084490947425365, 'learning_rate': 4.531516183986372e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0393, 'grad_norm': 0.009425430558621883, 'learning_rate': 4.488926746166951e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0002, 'grad_norm': 0.007563957013189793, 'learning_rate': 4.44633730834753e-05, 'epoch': 1.13}\n",
      "{'loss': 0.131, 'grad_norm': 0.027993520721793175, 'learning_rate': 4.4037478705281095e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0036, 'grad_norm': 0.03763426095247269, 'learning_rate': 4.3611584327086886e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0873, 'grad_norm': 0.024944884702563286, 'learning_rate': 4.3185689948892676e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0979, 'grad_norm': 0.04288553074002266, 'learning_rate': 4.275979557069847e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0548, 'grad_norm': 0.021005509421229362, 'learning_rate': 4.2333901192504264e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0204, 'grad_norm': 0.005821201950311661, 'learning_rate': 4.1908006814310054e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0053268619813025, 'learning_rate': 4.1482112436115845e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0604, 'grad_norm': 0.14922136068344116, 'learning_rate': 4.1056218057921635e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0038157349918037653, 'learning_rate': 4.0630323679727426e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0035028134007006884, 'learning_rate': 4.020442930153322e-05, 'epoch': 1.31}\n",
      "{'loss': 0.1011, 'grad_norm': 0.0035987200681120157, 'learning_rate': 3.977853492333901e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0003, 'grad_norm': 0.009445243515074253, 'learning_rate': 3.9352640545144804e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0021, 'grad_norm': 0.010792231187224388, 'learning_rate': 3.89267461669506e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0062, 'grad_norm': 0.007058991119265556, 'learning_rate': 3.850085178875639e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0004, 'grad_norm': 0.021089140325784683, 'learning_rate': 3.807495741056218e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0008, 'grad_norm': 0.009256266057491302, 'learning_rate': 3.764906303236798e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0035437028855085373, 'learning_rate': 3.722316865417377e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0912, 'grad_norm': 0.013083242811262608, 'learning_rate': 3.679727427597956e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0845, 'grad_norm': 0.201470285654068, 'learning_rate': 3.637137989778535e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0007, 'grad_norm': 0.007534398231655359, 'learning_rate': 3.594548551959114e-05, 'epoch': 1.49}\n",
      "{'loss': 0.1296, 'grad_norm': 0.027806395664811134, 'learning_rate': 3.551959114139694e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0566, 'grad_norm': 0.019021905958652496, 'learning_rate': 3.509369676320273e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0006, 'grad_norm': 0.0033966097980737686, 'learning_rate': 3.466780238500852e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0244, 'grad_norm': 0.014952941797673702, 'learning_rate': 3.424190800681431e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0004, 'grad_norm': 0.003241480328142643, 'learning_rate': 3.38160136286201e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0006, 'grad_norm': 0.010121083818376064, 'learning_rate': 3.33901192504259e-05, 'epoch': 1.59}\n",
      "{'loss': 0.1012, 'grad_norm': 0.01741579920053482, 'learning_rate': 3.296422487223169e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0006, 'grad_norm': 0.011487151496112347, 'learning_rate': 3.253833049403748e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0008, 'grad_norm': 0.01395210437476635, 'learning_rate': 3.2112436115843275e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0004, 'grad_norm': 0.021927734836935997, 'learning_rate': 3.1686541737649065e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0004, 'grad_norm': 0.023274362087249756, 'learning_rate': 3.1260647359454856e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0059903166256845, 'learning_rate': 3.083475298126065e-05, 'epoch': 1.7}\n",
      "{'loss': 0.1806, 'grad_norm': 0.015127216465771198, 'learning_rate': 3.0408858603066443e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0006, 'grad_norm': 0.020771263167262077, 'learning_rate': 2.9982964224872234e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0012, 'grad_norm': 0.04448052868247032, 'learning_rate': 2.9557069846678027e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0892, 'grad_norm': 0.042399812489748, 'learning_rate': 2.9131175468483818e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0821, 'grad_norm': 4.867120265960693, 'learning_rate': 2.8705281090289608e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0016, 'grad_norm': 0.07188699394464493, 'learning_rate': 2.8279386712095402e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0301, 'grad_norm': 0.043025463819503784, 'learning_rate': 2.7853492333901192e-05, 'epoch': 1.83}\n",
      "{'loss': 0.001, 'grad_norm': 0.018392447382211685, 'learning_rate': 2.7427597955706986e-05, 'epoch': 1.85}\n",
      "{'loss': 0.0007, 'grad_norm': 0.01687156781554222, 'learning_rate': 2.7001703577512777e-05, 'epoch': 1.86}\n",
      "{'loss': 0.0003, 'grad_norm': 0.007952079176902771, 'learning_rate': 2.6575809199318567e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0003, 'grad_norm': 0.006842820905148983, 'learning_rate': 2.614991482112436e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0003, 'grad_norm': 0.00855197012424469, 'learning_rate': 2.572402044293015e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0002, 'grad_norm': 0.004300045780837536, 'learning_rate': 2.5298126064735945e-05, 'epoch': 1.94}\n",
      "{'loss': 0.0002, 'grad_norm': 0.016978204250335693, 'learning_rate': 2.487223168654174e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0002, 'grad_norm': 0.004908478818833828, 'learning_rate': 2.444633730834753e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0005, 'grad_norm': 0.004001221619546413, 'learning_rate': 2.4020442930153323e-05, 'epoch': 1.99}\n",
      "{'loss': 0.0002, 'grad_norm': 0.004394433461129665, 'learning_rate': 2.3594548551959114e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0368, 'grad_norm': 0.0028401368763297796, 'learning_rate': 2.3168654173764907e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0033741286024451256, 'learning_rate': 2.27427597955707e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0021316467318683863, 'learning_rate': 2.2316865417376492e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0003, 'grad_norm': 0.002658870769664645, 'learning_rate': 2.1890971039182286e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0001, 'grad_norm': 0.002231622114777565, 'learning_rate': 2.1465076660988076e-05, 'epoch': 2.1}\n",
      "{'loss': 0.031, 'grad_norm': 0.002279475564137101, 'learning_rate': 2.1039182282793866e-05, 'epoch': 2.11}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0028834936674684286, 'learning_rate': 2.061328790459966e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0865, 'grad_norm': 0.003292154986411333, 'learning_rate': 2.018739352640545e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0001, 'grad_norm': 0.002535990672186017, 'learning_rate': 1.9761499148211244e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0032503220718353987, 'learning_rate': 1.9335604770017038e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0024986553471535444, 'learning_rate': 1.890971039182283e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0014556045643985271, 'learning_rate': 1.8483816013628622e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0017645700136199594, 'learning_rate': 1.8057921635434413e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0021352372132241726, 'learning_rate': 1.7632027257240207e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0025430552195757627, 'learning_rate': 1.7206132879045997e-05, 'epoch': 2.28}\n",
      "{'loss': 0.077, 'grad_norm': 1.0095782279968262, 'learning_rate': 1.6780238500851788e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0045937905088067055, 'learning_rate': 1.635434412265758e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0022523554507642984, 'learning_rate': 1.5928449744463375e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0025874637067317963, 'learning_rate': 1.5502555366269166e-05, 'epoch': 2.35}\n",
      "{'loss': 0.0002, 'grad_norm': 0.0017182854935526848, 'learning_rate': 1.507666098807496e-05, 'epoch': 2.37}\n",
      "{'loss': 0.0001, 'grad_norm': 0.018476279452443123, 'learning_rate': 1.465076660988075e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0025424195919185877, 'learning_rate': 1.4224872231686542e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0014344080118462443, 'learning_rate': 1.3798977853492334e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00207160459831357, 'learning_rate': 1.3373083475298126e-05, 'epoch': 2.44}\n",
      "{'loss': 0.0001, 'grad_norm': 0.002079747151583433, 'learning_rate': 1.2947189097103918e-05, 'epoch': 2.46}\n",
      "{'loss': 0.0396, 'grad_norm': 137.77255249023438, 'learning_rate': 1.252129471890971e-05, 'epoch': 2.47}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0025902551133185625, 'learning_rate': 1.2095400340715503e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0016106992261484265, 'learning_rate': 1.1669505962521295e-05, 'epoch': 2.51}\n",
      "{'loss': 0.0004, 'grad_norm': 0.002154530957341194, 'learning_rate': 1.1243611584327088e-05, 'epoch': 2.53}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0016119220526888967, 'learning_rate': 1.0817717206132879e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0016116175102069974, 'learning_rate': 1.0391822827938671e-05, 'epoch': 2.56}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0014463192783296108, 'learning_rate': 9.965928449744463e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0001, 'grad_norm': 0.021503755822777748, 'learning_rate': 9.540034071550257e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0021004537120461464, 'learning_rate': 9.114139693356049e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0001, 'grad_norm': 0.001870304229669273, 'learning_rate': 8.68824531516184e-06, 'epoch': 2.63}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0012547174701467156, 'learning_rate': 8.262350936967632e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0011412884341552854, 'learning_rate': 7.836456558773425e-06, 'epoch': 2.67}\n",
      "{'loss': 0.0001, 'grad_norm': 0.001846984843723476, 'learning_rate': 7.410562180579217e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0022672514896839857, 'learning_rate': 6.984667802385009e-06, 'epoch': 2.71}\n",
      "{'loss': 0.0001, 'grad_norm': 0.001370083075016737, 'learning_rate': 6.558773424190801e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0017944396240636706, 'learning_rate': 6.132879045996593e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0017551063792780042, 'learning_rate': 5.706984667802386e-06, 'epoch': 2.76}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0404893197119236, 'learning_rate': 5.281090289608177e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0001, 'grad_norm': 0.001806843327358365, 'learning_rate': 4.85519591141397e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0016934683080762625, 'learning_rate': 4.4293015332197615e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0402, 'grad_norm': 0.0014861911768093705, 'learning_rate': 4.0034071550255544e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00193549576215446, 'learning_rate': 3.577512776831346e-06, 'epoch': 2.85}\n",
      "{'loss': 0.0001, 'grad_norm': 0.001676731277257204, 'learning_rate': 3.151618398637138e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0014215150149539113, 'learning_rate': 2.7257240204429304e-06, 'epoch': 2.89}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0018960225861519575, 'learning_rate': 2.2998296422487225e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0781, 'grad_norm': 0.002095800591632724, 'learning_rate': 1.8739352640545146e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0017477627843618393, 'learning_rate': 1.4480408858603067e-06, 'epoch': 2.94}\n",
      "{'loss': 0.0001, 'grad_norm': 0.002009208081290126, 'learning_rate': 1.0221465076660988e-06, 'epoch': 2.96}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0018327422440052032, 'learning_rate': 5.962521294718911e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00233192415907979, 'learning_rate': 1.7035775127768315e-07, 'epoch': 2.99}\n",
      "{'train_runtime': 1321.2296, 'train_samples_per_second': 10.12, 'train_steps_per_second': 1.267, 'train_loss': 0.05949286560165417, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1674, training_loss=0.05949286560165417, metrics={'train_runtime': 1321.2296, 'train_samples_per_second': 10.12, 'train_steps_per_second': 1.267, 'total_flos': 1635347236816440.0, 'train_loss': 0.05949286560165417, 'epoch': 3.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load model for binary classification (2 classes: Spam and Ham)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Directory for model output\n",
    "    num_train_epochs=3,              # Number of training periods\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of steps for warm-up\n",
    "    weight_decay=0.01,               # L2 Regularisation\n",
    "    logging_dir='./logs',            # Directory for logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Create a trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate the model\n",
    "\n",
    "After training, we can evaluate the model to check its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da7df043e3d4f18b446852d2778e91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.06995370239019394, 'eval_runtime': 17.525, 'eval_samples_per_second': 63.624, 'eval_steps_per_second': 3.994, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate test data\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Save and load model\n",
    "\n",
    "After training, we can save the model, load and use it again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_pretrained('./spam_classifier_model')\n",
    "tokenizer.save_pretrained('./spam_classifier_model')\n",
    "\n",
    "# Load model\n",
    "model = BertForSequenceClassification.from_pretrained('./spam_classifier_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./spam_classifier_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Make predictions\n",
    "\n",
    "Finally, we can use the trained model to make predictions. Here is an example of how we could do this for a new email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 1 is: spam\n",
      "Message 2 is: ham\n",
      "Message 3 is: ham\n",
      "Message 4 is: spam\n",
      "Message 5 is: spam\n",
      "Message 6 is: spam\n",
      "Message 7 is: ham\n",
      "Message 8 is: spam\n",
      "Message 9 is: spam\n",
      "Message 10 is: ham\n"
     ]
    }
   ],
   "source": [
    "def predict(message):\n",
    "    # return_tensors='pt' --> specifies that tokenization should take place in PyTorch tensors (pt). Tokenized data for input into the model is returned as PyTorch tensors (and not as NumPy arrays or other formats)\n",
    "    inputs = tokenizer(message, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # model is called in order to make a prediction. The tokenized inputs are passed as input for the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # logits represent the improbability or the measure of how certain the model is in relation to each class\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # function returns index of the largest logit, which corresponds to the class that the model considers most likely\n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "    return 'spam' if prediction == 1 else 'ham'\n",
    "\n",
    "# Example 1\n",
    "message1 = \"Congratulations! You've won a free vacation!\"\n",
    "print(\"Message 1 is: \" + predict(message1))\n",
    "\n",
    "# Example 2\n",
    "message2 = \"Please call me back, in love, Tracy.\"\n",
    "print(\"Message 2 is: \" + predict(message2))\n",
    "\n",
    "# Example 3\n",
    "message3 = \"Hey Honey, we've won a free trip to Las Vegas! Isn't this awesome? Call me back soon, so we can discuss our planings for the trip.\"\n",
    "print(\"Message 3 is: \" + predict(message3))\n",
    "\n",
    "# Example 4\n",
    "message4 = \"Hey Honey, I miss you, last night was soooo unbelievable. If you need real love, call 123456789. Now! I'am waiting for u.\"\n",
    "print(\"Message 4 is: \" + predict(message4))\n",
    "\n",
    "# Example 5\n",
    "message5 = \"Hi Mom. Here is my new whatsapp number. Please call me back today. Here is my number: 33445678789. I'am your child Clara.\"\n",
    "print(\"Message 5 is: \" + predict(message5))\n",
    "\n",
    "# Example 6\n",
    "message6 = \"Yor delifery is larte. Your DHL pakage wil come later. Please follo the link here: https://www.link.com for mor details.\"\n",
    "print(\"Message 6 is: \" + predict(message6))\n",
    "\n",
    "# Example 7\n",
    "message7 = \"Please call 911, I am in danger. Jutta.\"\n",
    "print(\"Message 7 is: \" + predict(message7))\n",
    "\n",
    "# Example 8\n",
    "message8 = \"YOU WON A FREE TRIP TO LAS VEGAS!\"\n",
    "print(\"Message 8 is: \" + predict(message8))\n",
    "\n",
    "# Example 9\n",
    "message9 = \"https://www.hotshit.com!\"\n",
    "print(\"Message 9 is: \" + predict(message9))\n",
    "\n",
    "# Example 10\n",
    "message10 = \"Hi, my name is Max and I am 10 years old. I want to be a trainee in your company.\"\n",
    "print(\"Message 10 is: \" + predict(message10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
